{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50d055c-e4c2-4052-b6e6-b0351211ea3f",
   "metadata": {},
   "source": [
    "# Viam Anomaly Detector Jupyter Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f4257-19a2-45e7-8e4e-ca2ad85923fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146d060-65b6-47d6-86c6-ac2c41a48d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Training script for an Isolation Forest model using data from app.viam.com\n",
    "############################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from onnx import ModelProto\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import to_onnx\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import os\n",
    "import bson\n",
    "from dotenv import load_dotenv\n",
    "from viam.app.viam_client import ViamClient\n",
    "from viam.rpc.dial import DialOptions\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b78b8-8e1b-4f35-96aa-7b548778f59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up / config\n",
    "load_dotenv()\n",
    "model_name = \"isolation_forest.onnx\"\n",
    "data_file = \"data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3dee7a-165c-477a-ae3a-df663b10e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Get data from app.viam.com\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "async def connect() -> ViamClient:\n",
    "    \"\"\"Connect to the VIAM cloud api\"\"\"\n",
    "    dial_options = DialOptions.with_api_key(\n",
    "        api_key=os.environ.get(\"API_KEY\"), api_key_id=os.environ.get(\"API_KEY_ID\")\n",
    "    )\n",
    "    return await ViamClient.create_from_dial_options(dial_options)\n",
    "\n",
    "\n",
    "async def download(limit: int = 10000):\n",
    "    \"\"\"Download the training data\"\"\"\n",
    "    logging.info(\"Downloading data...\")\n",
    "    viam_client = await connect()\n",
    "    data_client = viam_client.data_client\n",
    "\n",
    "    # Query the data set using MongoDB Query Language (MQL)\n",
    "    tabular_data = await data_client.tabular_data_by_mql(\n",
    "        organization_id=\"96b696a0-51b9-403b-ae0d-63753923652f\",  # os.getenv(\"ORGANIZATION_ID\"),\n",
    "        # The MQL aggregation pipeline extract and preprocess the data\n",
    "        # TODO: Set this to suit your data! https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\n",
    "        mql_binary=[\n",
    "            bson.dumps(\n",
    "                {\n",
    "                    \"$match\": {\n",
    "                        \"component_name\": \"fake-sensor\",\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            bson.dumps(\n",
    "                {\n",
    "                    \"$project\": {\n",
    "                        \"timestamp\": {\"$dateToString\": {\"date\": \"$time_received\"}},\n",
    "                        \"value\": \"$data.readings.a\",\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            bson.dumps({\"$limit\": limit}),\n",
    "        ],\n",
    "    )\n",
    "    df = pd.DataFrame(tabular_data)\n",
    "    # TODO: timestamp conversion shouldn't be necessary once the Viam SDK date type in the MQL API is fixed\n",
    "    # Also remove above in the MQL pipeline\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    viam_client.close()\n",
    "    logging.info(f\"{len(df)} reading(s) downloaded\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da68d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Save to CSV\n",
    "############################################################################################################\n",
    "\n",
    "def save_to_csv(tabular_data):\n",
    "    df = pd.DataFrame(tabular_data)\n",
    "    df.to_csv(data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236548a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Load from CSV\n",
    "############################################################################################################\n",
    "\n",
    "def loadCSV():\n",
    "    print(\"Loading CSV...\")\n",
    "    df = pd.read_csv(data_file, parse_dates=[\"timestamp\"])\n",
    "    print(\"CSV Loaded\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608082b0-f0dd-4a43-8cea-bf8753342f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Feature Engineering\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "def featureEng(df: pd.DataFrame):\n",
    "    logging.info(\"Feature Engineering...\")\n",
    "    # A variety of resamples which I may or may not use\n",
    "    # TODO: Push down to the backend -> Agg. pipeline\n",
    "    df_sampled = df.set_index(\"timestamp\").resample(\"5min\").mean().reset_index()\n",
    "    # df_sampled = df.set_index(\"timestamp\").resample(\"h\").mean().reset_index()\n",
    "    # df_sampled = df.set_index(\"timestamp\").resample(\"D\").mean().reset_index()\n",
    "    # df_sampled = df.set_index(\"timestamp\").resample(\"W\").mean().reset_index()\n",
    "\n",
    "    # Calculate the rolling mean and lag (weekdays are not used in the model)\n",
    "    for DataFrame in [df_sampled]:\n",
    "        DataFrame[\"Weekday\"] = pd.Categorical(\n",
    "            DataFrame[\"timestamp\"].dt.strftime(\"%A\"),\n",
    "            categories=[\n",
    "                \"Monday\",\n",
    "                \"Tuesday\",\n",
    "                \"Wednesday\",\n",
    "                \"Thursday\",\n",
    "                \"Friday\",\n",
    "                \"Saturday\",\n",
    "                \"Sunday\",\n",
    "            ],\n",
    "        )\n",
    "        DataFrame[\"Hour\"] = DataFrame[\"timestamp\"].dt.hour\n",
    "        DataFrame[\"Day\"] = DataFrame[\"timestamp\"].dt.weekday\n",
    "        DataFrame[\"Month\"] = DataFrame[\"timestamp\"].dt.month\n",
    "        DataFrame[\"Year\"] = DataFrame[\"timestamp\"].dt.year\n",
    "        DataFrame[\"Month_day\"] = DataFrame[\"timestamp\"].dt.day\n",
    "        DataFrame[\"Lag\"] = DataFrame[\"value\"].shift(1)\n",
    "        DataFrame[\"Rolling_Mean\"] = DataFrame[\"value\"].rolling(7, min_periods=1).mean()\n",
    "        DataFrame = DataFrame.dropna()\n",
    "    df_sampled.dropna(inplace=True)\n",
    "    logging.info(\"Feature Engineering Completed\")\n",
    "    return df_sampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c40a95-b064-4403-aba8-a0e841f9d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################################################################\n",
    "# Model Training\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "def fit_isolation_forest(\n",
    "    model_data: pd.DataFrame,\n",
    ") -> ModelProto:\n",
    "    logging.info(\"Fitting Isolation Forest...\")\n",
    "    model_data = (\n",
    "        model_data[\n",
    "            [\n",
    "                \"value\",\n",
    "                \"Hour\",\n",
    "                \"Day\",\n",
    "                \"Month_day\",\n",
    "                \"Month\",\n",
    "                \"Rolling_Mean\",\n",
    "                \"Lag\",\n",
    "                \"timestamp\",\n",
    "            ]\n",
    "        ]\n",
    "        .set_index(\"timestamp\")\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    IF = IsolationForest(\n",
    "        # Parameter tuning:\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest\n",
    "        random_state=0,\n",
    "        contamination=\"auto\",\n",
    "        n_estimators=100,\n",
    "        max_samples=\"auto\",\n",
    "    )\n",
    "\n",
    "    IF.fit(model_data)\n",
    "\n",
    "    onx = to_onnx(\n",
    "        IF,\n",
    "        model_data.to_numpy().astype(np.float32),\n",
    "        target_opset={\"\": 15, \"ai.onnx.ml\": 3},\n",
    "        initial_types=[(\"observation\", FloatTensorType([None, 7]))],\n",
    "        final_types=[\n",
    "            (\"label\", FloatTensorType([None, 1])),\n",
    "            (\"scores\", FloatTensorType([None, 1])),\n",
    "        ],\n",
    "    )\n",
    "    return onx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Model inference for validation purposes\n",
    "############################################################################################################\n",
    "\n",
    "# https://onnxruntime.ai/docs/api/python/tutorial.html\n",
    "\n",
    "from onnxruntime import InferenceSession, get_available_providers\n",
    "\n",
    "def inference(df: pd.DataFrame):\n",
    "    print(\"Inference...\")\n",
    "    model_data = (\n",
    "        df[\n",
    "            [\n",
    "                \"value\",\n",
    "                \"Hour\",\n",
    "                \"Day\",\n",
    "                \"Month_day\",\n",
    "                \"Month\",\n",
    "                \"Rolling_Mean\",\n",
    "                \"Lag\",\n",
    "                \"timestamp\",\n",
    "            ]\n",
    "        ]\n",
    "        .set_index(\"timestamp\")\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    ort_sess = InferenceSession(\n",
    "        model_name,\n",
    "        providers=get_available_providers(),\n",
    "    )\n",
    "\n",
    "    nparray = model_data.to_numpy(dtype=np.float32)\n",
    "\n",
    "    input_name = ort_sess.get_inputs()[0].name\n",
    "    output_name = ort_sess.get_outputs()[0].name\n",
    "    print(\"Input Name:\", input_name)\n",
    "    print(\"Output Name:\", output_name)\n",
    "\n",
    "    pred_onx = ort_sess.run(None, {input_name: nparray})\n",
    "\n",
    "    inference = pd.DataFrame(\n",
    "        np.column_stack([pred_onx[0], pred_onx[1]]), columns=[\"outlier\", \"score\"]\n",
    "    )\n",
    "    print(\"Inference Completed\")\n",
    "    return inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efe220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "training_data = await download()\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df = featureEng(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce43389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "onx = fit_isolation_forest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e662b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "with open(model_name, \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the training data\n",
    "\n",
    "inference(df)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
